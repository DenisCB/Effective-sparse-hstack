{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import itertools\n",
    "from scipy import sparse\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%pylab inline\n",
    "import matplotlib.pylab as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('ggplot')\n",
    "plt.style.use('seaborn-poster')\n",
    "sns.set_palette('Set1', 10, desat=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def batched_hstack_csr(matrices, batch_size=-1):\n",
    "    \"\"\"\n",
    "    Scipy sparse hstack operation works in linear time only on csc matrices or small csr matrices. \n",
    "    So in this function matrices hstacked by small batches, and then the batches are stacked vertically. Vertical\n",
    "    stacking for csr matrices is a cheap operation.\n",
    "    \n",
    "    Input: \n",
    "        matrices: list of matrices to be hstacked. All must have the same number of rows. \n",
    "            Acceptable formats: csr matrix, pandas DataFrame or numpy array.\n",
    "        batch_size: int, number of rows to hstack per batch. If not defined then batch_size is set equal to \n",
    "            the number of batches, i.e. square root of number of rows in matricies. Recommended batch size 1K - 10K.\n",
    "    Output: scipy csr matrix consisting of matrices stacked horizontally   \n",
    "    \"\"\"\n",
    "    if batch_size == -1 or batch_size == 'dynamic':\n",
    "        batch_size = np.ceil(np.sqrt(matrices[0].shape[0]))\n",
    "     \n",
    "    if batch_size == 0 or batch_size >= matrices[0].shape[0]:\n",
    "        return sparse.hstack(matrices, format='csr')\n",
    "    \n",
    "    batch_size = int(batch_size)\n",
    "    batches = []\n",
    "    for i in range(0, matrices[0].shape[0], batch_size):\n",
    "        lower_bound = i\n",
    "        upper_bound = min(i+batch_size, matrices[0].shape[0])\n",
    "        batches.append(sparse.hstack([matrix[lower_bound:upper_bound] for matrix in matrices]\n",
    "                                     , format='csr'))\n",
    "    \n",
    "    return sparse.vstack(batches, format='csr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dummy data creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dense data is generated via np.random.uniform. \n",
    "\n",
    "In order to generate sparse data we'll generate *NROWS* random sentences where each word is an integer from range (0, *NCOLS_SPARSE*) converted into a string. Number of words in each sentence equals to *NCOLS_SPARSE* \\* *SPARSE_DENSITY*. Resulted coprus is then transformed via CountVectorizer to sparse matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# init parameters\n",
    "NROWS = int(2e6)\n",
    "NCOLS_DENSE = 200\n",
    "NCOLS_SPARSE = int(1e5) # approximate \n",
    "SPARSE_DENSITY = 2e-4\n",
    "\n",
    "# Dense data creation\n",
    "data_dense = np.random.uniform(size=[NROWS, NCOLS_DENSE])\n",
    "\n",
    "# Sparse data creation. \n",
    "words_in_sentence = int(NCOLS_SPARSE*SPARSE_DENSITY)\n",
    "corpus = np.random.choice(size=[NROWS, words_in_sentence], a=NCOLS_SPARSE)\n",
    "corpus = list((map(lambda x: ' '.join(x.astype(str)), corpus)))\n",
    "print ('Example of 3 sentences for sparse matrix creation:')\n",
    "for sentence in corpus[:3]:\n",
    "    print (sentence)\n",
    "print()\n",
    "\n",
    "vectorizer = CountVectorizer().fit(corpus) \n",
    "data_sparse = vectorizer.transform(corpus)\n",
    "\n",
    "print ('Dense  data shape:', data_dense.shape)\n",
    "print ('Sparse data shape:', data_sparse.shape, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# check whether function returns the same result as default hstack\n",
    "assert (sparse.hstack([data_sparse[:200], data_dense[:200]], format='csr') \n",
    "        == batched_hstack_csr([data_sparse[:200], data_dense[:200]])).toarray().all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Hstacking two sparse matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "number_of_trials = 4 #  Number of trials for each matrix size. Is used to calculate 95% confedence intervals\n",
    "nrows_list = np.array([1e4, 2.5e4, 5e4, 1e5, 2.5e5, 5e5, 7.5e5, 1e6, 1.25e6, 1.5e6, 1.75e6, 2e6]).astype(int)\n",
    "\n",
    "default_time_means = []\n",
    "default_time_stds  = []\n",
    "\n",
    "batched_time_means = []\n",
    "batched_time_stds  = []\n",
    "\n",
    "for nrows in nrows_list:\n",
    "    print ('Started on {} row'.format(nrows)) \n",
    "    \n",
    "    default_trials = []\n",
    "    batched_trials = []\n",
    "    \n",
    "    for trial_num in range(number_of_trials):\n",
    "        \n",
    "        # Measuring scipy sparse hstack performance\n",
    "        t_start = time.time()\n",
    "        _ = sparse.hstack([data_sparse[:nrows], data_sparse[:nrows]], format='csr')\n",
    "        default_trials.append(time.time() - t_start)\n",
    "        \n",
    "        # Measuring batched scipy sparse hstack performance\n",
    "        t_start = time.time()\n",
    "        _ = batched_hstack_csr([data_sparse[:nrows], data_sparse[:nrows]])\n",
    "        batched_trials.append(time.time() - t_start)\n",
    "        \n",
    "    default_time_means.append(np.mean(default_trials))\n",
    "    batched_time_means.append(np.mean(batched_trials))\n",
    "    \n",
    "    default_time_stds.append( 2*np.std(default_trials, ddof=1)/np.sqrt(number_of_trials) )\n",
    "    batched_time_stds.append( 2*np.std(batched_trials, ddof=1)/np.sqrt(number_of_trials) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,7))\n",
    "\n",
    "plt.errorbar(x=nrows_list, y=batched_time_means, label='batched',\n",
    "             yerr=batched_time_stds, capthick=1, capsize=5, elinewidth=1)\n",
    "\n",
    "plt.errorbar(x=nrows_list, y=default_time_means, label='default',\n",
    "             yerr=default_time_stds, capthick=1, capsize=5, elinewidth=1)\n",
    "\n",
    "plt.title('Stacking performance for batched and default scipy stackings')\n",
    "plt.ylabel('Seconds')\n",
    "plt.xlabel('Number of rows')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Hstacking dense and sparse matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "number_of_trials = 4\n",
    "nrows_list = np.array([1e4, 5e4, 1e5, 2.5e5, 5e5, 7.5e5, 1e6, 1.25e6, 1.5e6]).astype(int)\n",
    "\n",
    "default_time_means = []\n",
    "default_time_stds = []\n",
    "\n",
    "batched_time_means = []\n",
    "batched_time_stds = []\n",
    "\n",
    "for nrows in nrows_list:\n",
    "    print ('Started on {} row'.format(nrows)) \n",
    "    \n",
    "    default_trials = []\n",
    "    batched_trials = []\n",
    "    for trial_num in range(number_of_trials):\n",
    "\n",
    "        t_start = time.time()\n",
    "        _ = sparse.hstack([data_dense[:nrows], data_sparse[:nrows]], format='csr')\n",
    "        default_trials.append(time.time() - t_start)\n",
    "        \n",
    "        t_start = time.time()\n",
    "        _ = batched_hstack_csr([data_dense[:nrows], data_sparse[:nrows]])\n",
    "        batched_trials.append(time.time() - t_start)\n",
    "        \n",
    "    default_time_means.append(np.mean(default_trials))\n",
    "    batched_time_means.append(np.mean(batched_trials))\n",
    "    \n",
    "    default_time_stds.append( 2*np.std(default_trials, ddof=1)/np.sqrt(number_of_trials) )\n",
    "    batched_time_stds.append( 2*np.std(batched_trials, ddof=1)/np.sqrt(number_of_trials) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,7))\n",
    "\n",
    "plt.errorbar(x=nrows_list, y=batched_time_means, label='batched',\n",
    "             yerr=batched_time_stds, capthick=1, capsize=5, elinewidth=1)\n",
    "\n",
    "plt.errorbar(x=nrows_list, y=default_time_means, label='default',\n",
    "             yerr=default_time_stds, capthick=1, capsize=5, elinewidth=1)\n",
    "\n",
    "plt.title('Stacking performance for batched and default scipy stackings')\n",
    "plt.ylabel('Seconds')\n",
    "plt.xlabel('Number of rows')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch size influence on performance, closer look on small datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "number_of_trials = 5\n",
    "nrows_list = np.array([1e4, 5e4, 1e5, 2.5e5, 5e5, 7.5e5, 1e6, 1.25e6, 1.5e6]).astype(int)\n",
    "\n",
    "batches_list = ['dynamic', 100, 1000, 10000, 100000]\n",
    "\n",
    "batched_time_means = dict()\n",
    "batched_time_stds = dict()\n",
    "\n",
    "iter_num = 0\n",
    "for nrows, batch_size in itertools.product(nrows_list, batches_list):\n",
    "    if iter_num % len(batches_list)==0:\n",
    "        print ('Started on {} row'.format(nrows)) \n",
    "    \n",
    "    batched_time_means.setdefault(batch_size, [])\n",
    "    batched_time_stds .setdefault(batch_size, [])        \n",
    "    \n",
    "    trials = []\n",
    "    for trial_num in range(number_of_trials):\n",
    "        \n",
    "        t_start = time.time()\n",
    "        _ = batched_hstack_csr([data_dense[:nrows], data_sparse[:nrows]], batch_size=batch_size)\n",
    "        trials.append(time.time() - t_start)\n",
    "    \n",
    "    batched_time_means[batch_size].append(np.mean(trials))\n",
    "    batched_time_stds [batch_size].append( 2*np.std(trials, ddof=1)/np.sqrt(number_of_trials) )\n",
    "    \n",
    "    iter_num+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,7))\n",
    "\n",
    "for batch_size in batches_list:\n",
    "    \n",
    "    plt.errorbar(x=nrows_list, y=batched_time_means[batch_size], label=batch_size,\n",
    "                 yerr=batched_time_stds[batch_size], capthick=1, capsize=5, elinewidth=1)\n",
    "\n",
    "plt.legend()\n",
    "plt.title('Hstacking performance for different batch sizes')\n",
    "plt.ylabel('Seconds')\n",
    "plt.xlabel('Number of rows')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can't really see anything on the graph above. Let's plot bar plots for every dataset size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "graph_nrows = np.ceil(nrows_list.shape[0]/2)\n",
    "plt.figure(figsize=(15, 5*graph_nrows))\n",
    "nrows_list = np.array([1e4, 5e4, 1e5]).astype(int)\n",
    "\n",
    "\n",
    "for nrows_index in range(nrows_list.shape[0]):\n",
    "    bar_heights = [batched_time_means[batch_size][nrows_index] for batch_size in batches_list]\n",
    "    bar_heights.append(default_time_means[nrows_index])\n",
    "    \n",
    "    bar_yerrors = [batched_time_stds[batch_size][nrows_index] for batch_size in batches_list] \n",
    "    bar_yerrors.append(default_time_stds[nrows_index])\n",
    "    \n",
    "    colors = [sns.color_palette()[2]] + [sns.color_palette()[1] for i in batches_list[:-1]] + [sns.color_palette()[0]]\n",
    "    \n",
    "    plt.subplot(graph_nrows, 2, nrows_index+1)\n",
    "    plt.bar(left = range(len(bar_heights))\n",
    "           ,height = bar_heights\n",
    "           ,yerr = bar_yerrors\n",
    "           ,capsize = 10\n",
    "           ,error_kw = {'elinewidth':1, 'capsize':10, 'capthick':1}\n",
    "           ,color = colors\n",
    "           )\n",
    "    plt.title('Hstacking times for {} K rows'.format(int(nrows_list[nrows_index]/1000)))\n",
    "    plt.ylabel('Seconds')\n",
    "    plt.xlabel('Batch size')\n",
    "    plt.ylim(0, np.max(bar_heights[:-1])*1.3)\n",
    "    plt.xticks(range(len(bar_heights)), batches_list+['default'])\n",
    "    \n",
    "plt.tight_layout(pad=3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "1) Batched horizontal csr stacking with dynamic batch size is always faster than default scipy sparse hstack.\n",
    "\n",
    "2) Dynamic batch size almost always at least not worse than any other batch size, except for the data of small size. In case of small data smaller batches show approximately the same performance.\n",
    "\n",
    "3) With dataset size increasing, the performance of basic scipy sparse stacking is increasing non-linearly. When using batched hstacking, compelxity growth almost lineray along with dataset size. For example, in Quora competition scipy sparse hstacking was crashing because of MemoryError after 70-90 minutes of computations. Batched function successfully completed the same task within 10 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
